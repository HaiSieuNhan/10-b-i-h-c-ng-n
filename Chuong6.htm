<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <link href="./style/bootstrap.css" type="text/css" rel="stylesheet"/>
    <script src="./style/bootstrap.js"></script>
    <link href="./style/style.css" type="text/css" rel="stylesheet"/>

</head>
<body>

    <div class="container">
        <p class="title-chuong">06 THAY ĐỔI ĐỂ HOÀN THIỆN</p>
        <p class = "quote">“Chỉ những người có giáo dục mới biết cách học và thay đổi.” CARL ROGERS</p>
        <p>Một tòa tháp bằng các khối gỗ đứng ngay trước mặt chúng ta. Một cánh tay robot với bàn tay là một cái kẹp từ từ di chuyển xung quanh tháp, chọc vào và đẩy các khối khác nhau ra ngoài. Nó dừng lại ở một khối gỗ và cẩn thận đẩy nó ra một nửa, hãm bớt chuyển động của khối gỗ bằng một cái lắc nhẹ như con người vẫn thường làm. Sau đó, nó di chuyển sang phía bên kia, nhẹ nhàng kéo khối gỗ ra và đặt lên đỉnh tháp. Sau đó, robot quay trở lại và bắt đầu đi vòng quanh tháp một lần nữa, chọc nhẹ vào các khối gỗ cho đến khi cảm nhận được một khối khác mà nó muốn. Đây không phải là cánh tay robot bình thường. Đó là một robot đã học được cách cảm nhận về nhiệm vụ, đánh giá các lực tác động và phản hồi để đưa ra quyết định về hành động cần thực hiện. Nó là một robot tự học.</p>
        <p class = "title">&nbsp;Tự học cách học</p>
        <p>&nbsp;Trí tuệ nhân tạo có thể gây kinh ngạc khi học các trò chơi thuần túy lý thuyết, từ cờ vua và cờ vây đến trò chơi điện tử. Nhưng khi đặt hầu hết các robot trước trò chơi Jenga (một tháp gỗ mà từ đó bạn rút các khối gỗ dưới thấp ra rồi đặt lên đỉnh tháp), bạn sẽ thu được kết quả rất lộn xộn. Ngay cả khi các robot được huấn luyện bằng cách sử dụng phương pháp học có giám sát trong thế giới mô phỏng, thì mức độ phức tạp và khả năng biến đổi của thế giới thực luôn quá khác biệt. Cách thông thường để huấn luyện AI hiểu được thực tế là cho nó xem hàng triệu ví dụ về những nỗ lực thành công và thất bại trong việc rút các khối gỗ thực. Cách tiếp cận như vậy sẽ mất một khoảng thời gian rất dài vì tháp cần được sắp xếp lại hàng triệu lần. Ngay cả sau đó, chỉ cần mỗi khối gỗ khác biệt đôi chút và các yếu tố không thể đoán trước như nhiệt độ và độ ẩm ảnh hưởng đến ma sát, những gì robot đã học được trước đó cũng có thể không có tác dụng vào lần sau.&nbsp;</p>
        <div style="text-align : center">
            <img src="./img/Anh25.png" alt="Ảnh 25: Cánh tay robot trước tháp gỗ" style="width : 40%"/>
            <p class = "title-img"><i>Ảnh 25: Cánh tay robot trước tháp gỗ</i> </p>
        <p>Đó là lý do tại sao Nima Fazeli và các đồng nghiệp của ông ở MIT đã phát triển một kiểu AI mới. Thay vì huấn luyện AI bằng cách sử dụng phương pháp học có giám sát, các nhà nghiên cứu đặt cánh tay robot trước tháp gỗ và để nó tự học qua trò chơi. Khi robot cẩn thận đẩy vào một khối, một máy tính nhận phản hồi trực quan và xúc giác từ máy ảnh và vòng bít của nó, đồng thời so sánh các phép đo này với các động tác mà robot đã thực hiện trước đó. Nó cũng xem xét kết quả của những động thái đó, cụ thể là một khối trong một cấu hình nhất định và được đẩy với một lượng lực nhất định, có được trích xuất thành công hay không. Trong thời gian thực, robot sau đó "học" xem có nên tiếp tục đẩy hay di chuyển đến một khối mới, để giữ cho tòa tháp không bị sụp đổ. "Không giống như trong các nhiệm vụ hoặc trò chơi nhận thức thuần túy hơn như cờ vua hoặc cờ vây, chơi trò chơi Jenga cũng đòi hỏi phải thành thạo các kỹ năng thể chất như thăm dò, đẩy, kéo, đặt và căn chỉnh các quân cờ. Nó đòi hỏi nhận thức và thao tác tương tác, nơi bạn phải đi và chạm vào tháp để tìm hiểu cách thức và thời điểm di chuyển các khối, "Rodriguez nói. "Điều này rất khó để mô phỏng, vì vậy robot phải học trong thế giới thực, bằng cách tương tác với tháp Jenga thực sự. Thách thức chính là học hỏi từ một số lượng tương đối nhỏ các thí nghiệm bằng cách khai thác lẽ thường về các vật thể và vật lý". Bằng cách đẩy, kéo và cảm nhận kết quả, robot mới có thể hiểu được hành động của nó sẽ ảnh hưởng như thế nào đến khối tháp không đều và dễ lung lay này. Chỉ sau khoảng 300 lần thử, nó đã nhóm các hành động của mình thành nhiều loại khác nhau; Để lập trình cho một robot chơi Jenga, các sơ đồ học máy truyền thống có thể yêu cầu nắm bắt mọi thứ có thể xảy ra giữa một khối, rô bốt và tháp - một nhiệm vụ tính toán tốn kém đòi hỏi dữ liệu từ hàng nghìn nếu không muốn nói là hàng chục nghìn nỗ lực trích xuất khối. Thay vào đó, Rodriguez và các đồng nghiệp của ông đã tìm kiếm một cách hiệu quả hơn về dữ liệu để một robot học cách chơi Jenga, lấy cảm hứng từ nhận thức của con người và cách chúng ta có thể tiếp cận trò chơi. Ví dụ, khối bị kẹt (tốt nhất là để yên đó), hoặc khối lỏng lẻo (có thể rút ra). AI Bayes “cảm nhận” được vấn đề theo đúng nghĩa đen và sau đó có thể khái quát hóa sự hiểu biết của mình cho tất cả các động tác được thực hiện trong tương lai. Những khả năng này có thể cải thiện robot của nhà máy bằng cách cho phép chúng hiểu cảm giác khi một bộ phận không được lắp vào đúng vị trí hoặc một con vít không được siết đúng cách. Nó có thể học được cảm giác về lực và tiếp xúc ngay cả khi mọi thứ thay đổi theo thời gian.</p>
        <p class = "quote">“Trò chơi Jenga… đòi hỏi bạn phải thành thạo các kỹ năng thể chất như dò tìm, đẩy, kéo, đặt và sắp xếp các khối gỗ.” ALBERTO RODRIGUEZ Giáo sư MIT (2019)</p>
        <p class = "title">Các loại học máy</p>
        <p>&nbsp;Phân cụm (Clustering) là một trong những hình thức học không giám sát được sử dụng phổ biến nhất. Thay vì dạy AI phân loại dữ liệu (có thể là “mèo” hoặc “chó”), chúng ta có thể không nắm được chút nào về cách làm việc này và yêu cầu máy tính tự tìm hiểu. Các nhà bán lẻ hẳn muốn hiểu khách hàng hơn. Nếu máy tính phát hiện ra rằng có năm loại khách hàng chính (các bà mẹ, thanh niên, người mua dịp cuối tuần, người thích giảm giá, người mua trung thành), và mỗi loại khách hàng mua sắm những thứ khác nhau vào những thời điểm khác nhau, nhà bán lẻ có thể đáp ứng tốt hơn nhu cầu cá nhân của họ thay vì đối xử với mọi người như nhau. Ý tưởng này cũng tạo cơ sở cho các hệ thống gợi ý (recommender system), có trách nhiệm tìm ra những điểm tương đồng giữa các khách hàng để giới thiệu sản phẩm mới cho họ. Nếu tôi giống bạn về độ tuổi, giới tính, quốc gia và đã đánh giá một số cuốn sách theo cách tương tự như bạn, thì khi tôi mua hoặc đánh giá cao một sản phẩm mới, sau này bạn có thể nhận được một gợi ý về sản phẩm tương tự. Kết hợp đầy đủ dữ liệu từ hàng nghìn hoặc hàng triệu người tiêu dùng sẽ tạo nên các khuyến nghị hợp lý một cách đáng kinh ngạc. Loại hệ thống gợi ý này được gọi là lọc cộng tác (collaborative filtering), có thể sử dụng các thuật toán phân cụm để nhóm các cá thể với nhau. Một hệ thống lọc cộng tác bắt đầu với một lịch sử sở thích của con người. Chức năng khoảng cách quyết định sự tương đồng phụ thuộc vào sự chồng chéo của sở thích những người thích cùng một thứ gần gũi.</p>
        <p>Hơn nữa, phiếu bầu được tính theo khoảng cách, do đó phiếu bầu của những người hàng xóm gần gũi hơn được tính nhiều hơn cho sự tán thành. Theo một thuật ngữ khác, đó là một cách tiếp cận để khám phá âm nhạc, sách, rượu vang hoặc một người nào đó khác phù hợp với sở thích hiện tại của một người cụ thể bằng cách sử dụng các đánh giá của một nhóm đồng đẳng chọn cho cùng sở thích của họ. Phương pháp này được gọi là lọc thông tin xã hội.</p>
        <p>Lọc cộng tác tự động hóa quy trình sử dụng truyền miệng để xác định xem họ có thể thích thứ gì đó hay không. Biết rằng một số người thích một cái gì đó là không đủ. Mọi người đều đánh giá cao một số đề xuất hơn những đề xuất khác. Lời giới thiệu của một người bạn thân có các đề xuất trước đó đã được tập trung đúng vào trọng tâm có thể đủ để bạn đi xem một bộ phim mới ngay cả khi nó thuộc thể loại mà nó thường có thể không thích.</p>
        <p>Chuẩn bị các đề xuất cho người dùng mới bằng cách sử dụng hệ thống lọc cộng tác tự động có ba bước như sau. Nó có thể là xây dựng hồ sơ người dùng bằng cách tiếp nhận khách hàng mới để xếp hạng lựa chọn các mặt hàng bao gồm phim, bài hát hoặc nhà hàng. Nó có thể là so sánh hồ sơ người dùng mới với hồ sơ của những người dùng khác bằng cách sử dụng một số thước đo tương tự. Nó có thể sử dụng một số kết hợp xếp hạng của người dùng có cùng hồ sơ để dự báo xếp hạng mà người dùng mới có thể cung cấp cho các mục mà họ chưa xếp hạng.</p>
        <p>Một thách thức với việc lọc cộng tác là có nhiều mục được xếp hạng hơn nhiều so với một người nào đó có khả năng đã hoàn thành hoặc sẵn sàng xếp hạng. Đó là, hồ sơ thường thưa thớt, xác định rằng có rất ít sự chồng chéo giữa sở thích của người dùng để tạo đề xuất. Hãy nghĩ về hồ sơ khách hàng như một vectơ với một thành phần cho mỗi mục trong vũ trụ của các yếu tố được xếp hạng. Mỗi phần tử của vectơ xác định xếp hạng của chủ sở hữu hồ sơ cho phần tử tương ứng theo thang điểm từ –5 đến 5 với 0 biểu thị tính trung lập và giá trị trống mà không có ý kiến.</p>
        <p>Nếu có hàng ngàn thành phần trong vectơ và mỗi người dùng quyết định xếp hạng thành phần nào, bất kỳ hồ sơ nào của hai người dùng đều có khả năng kết thúc với một số chồng chéo. Trong các thuật ngữ khác, việc buộc người dùng xếp hạng một tập hợp con cụ thể có thể bỏ lỡ dữ liệu thú vị vì xếp hạng của các yếu tố tối nghĩa hơn có thể nói lên nhiều điều về người dùng hơn là xếp hạng của những người nói chung.</p>
        <p>Hãy coi việc học tập không giám sát như một phiên bản toán học của việc khiến “những con chim cùng kiểu lông bay thành đàn với nhau”. CASSIE KOZYRKOV Kỹ sư trưởng, Google Cloud (2018)</p>
        <div style="text-align : center">
            <img src="./img/Anh26.png" alt="Ảnh 26: Sơ đồ tự tổ chức" style="width : 60%"/>
            <p class = "title-img"><i></i> </p>
        <p>Có rất nhiều hình thái của học không giám sát, và thậm chí là sự pha trộn giữa học có giám sát và không giám sát (chẳng hạn như học bán giám sát – semisupervised learning). Mặc dù đóng vai trò cực kỳ quan trọng trong các doanh nghiệp ngày nay để phân tích, phân loại và dự đoán, nhưng đối với việc điều khiển robot, phương pháp này vẫn gặp nhiều vấn đề. Các trở ngại thường phát sinh do sự chỉ định tín vụ (credit assignment). Nếu tôi là một robot được giao nhiệm vụ tìm ra con đường tốt nhất qua một cảnh quan phức tạp, tránh những chướng ngại vật chưa biết trước có thể gây tắc nghẽn, thì tôi phải đưa ra một chuỗi các quyết định. Thành công của quyết định sau sẽ phụ thuộc vào những quyết định trước đó – nếu rẽ trái để tránh cái hồ, thì bây giờ tôi phải tìm đường qua sông. Nếu rẽ phải để tránh hồ, tôi phải đi qua một đống đá. Việc học có giám sát không thể giúp gì được vì những trở ngại và trình tự của chúng không được biết trước và tôi không thể được huấn luyện về quyết định nào của mình có nhiều khả năng là đúng nhất và quyết định nào có thể làm tôi mắc kẹt. Học không giám sát, chẳng hạn như thuật toán phân nhóm, có thể giúp tôi phân loại các loại trở ngại mình nhìn thấy, nhưng một lần nữa, nó không cho phép tôi biết mình nên đi theo lộ trình nào. Tôi không có cách nào để xác định tính đúng đắn của (chỉ định tín vụ cho) từng quyết định đơn lẻ trong chuỗi lựa chọn phải thực hiện. Nếu không biết mình đang làm tốt như thế nào thì làm sao học được?</p>
        <p class = "title">&nbsp;Các nhà hoạch định chính sách</p>
        <p class = "quote">“Điều phi thường mà tôi nhận thấy là sự linh hoạt trong việc học tập của trẻ em – đối mặt với hầu hết mọi vấn đề đơn giản và chỉ sau một vài lần thử, bọn nhỏ đã giải quyết vấn đề ngày một tốt hơn. Làm sao để hoạt động của trẻ trở nên hiệu quả hơn chứ không phải kém đi?” CHRIS WATKINS (2005)</p>
        <p>Câu trả lời đến từ một loại hình học tập khác có tên là học tăng cường (reinforcement learning), phương pháp học được tạo ra lần đầu tiên vào những năm 1960 bởi các nhà nghiên cứu như John Andreae và Donald Michie. Con người xuất sắc trong việc giải quyết nhiều vấn đề thách thức, từ điều khiển vận động cấp thấp cho đến các nhiệm vụ nhận thức cấp cao. Mục tiêu của chúng tôi tại DeepMind là tạo ra các tác nhân nhân tạo có thể đạt được mức hiệu suất và tính tổng quát tương tự. Giống như con người, các nhân viên của chúng tôi tự học hỏi để đạt được các chiến lược thành công dẫn đến phần thưởng dài hạn lớn nhất. Mô hình học tập bằng cách thử và sai, chỉ từ phần thưởng hoặc hình phạt, được gọi là học tăng cường (RL). Cũng giống như một con người, các đại lý của chúng tôi xây dựng và học hỏi kiến thức của riêng họ trực tiếp từ các đầu vào thô, chẳng hạn như tầm nhìn, mà không cần bất kỳ tính năng thủ công hoặc heuristics miền nào. Điều này đạt được bằng cách học sâu các mạng nơ-ron. Tại DeepMind, chúng tôi đã đi tiên phong trong việc kết hợp các phương pháp tiếp cận này - học tăng cường sâu - để tạo ra các tác nhân nhân tạo đầu tiên đạt được hiệu suất cấp độ con người trên nhiều lĩnh vực đầy thách thức.</p>
        <p>Các đại lý của chúng tôi phải liên tục đưa ra các đánh giá giá trị để chọn hành động tốt hơn là xấu. Kiến thức này được thể hiện bằng một mạng Q ước tính tổng phần thưởng mà một tổng nhân viên có thể mong đợi nhận được sau khi thực hiện một hành động cụ thể. Hai năm trước, chúng tôi đã giới thiệu thuật toán thành công rộng rãi đầu tiên cho học tăng cường sâu. Ý tưởng chính là sử dụng các mạng nơ-ron sâu để đại diện cho mạng Q và đào tạo mạng Q này để dự đoán tổng phần thưởng. Những nỗ lực trước đây để kết hợp RL với các mạng nơ-ron phần lớn đã thất bại do việc học không ổn định. Để giải quyết những bất ổn này, thuật toán Deep Q-Networks (DQN) của chúng tôi lưu trữ tất cả trải nghiệm của tổng đài viên, sau đó lấy mẫu ngẫu nhiên và phát lại các trải nghiệm này để cung cấp dữ liệu đào tạo đa dạng và phù hợp. Chúng tôi đã áp dụng DQN để học cách chơi trò chơi trên bảng điều khiển Atari 2600. Ở mỗi bước thời gian, nhân viên quan sát các pixel thô trên màn hình, tín hiệu phần thưởng tương ứng với điểm số trò chơi và chọn hướng cần điều khiển. Trong bài báo Nature của chúng tôi, chúng tôi đã đào tạo các đại lý DQN riêng biệt cho 50 trò chơi Atari khác nhau mà không có bất kỳ kiến thức nào trước về các quy tắc trò chơi.</p>
        <p>Dạng AI thông minh này giống như một công cụ tối ưu hóa các chính sách hành vi (behavioural policy) – nó ước tính chất lượng khả dĩ của mỗi hành động tiềm năng trong một tình huống nhất định và học chuỗi hành động phù hợp để tạo ra kết quả mong muốn. Kỹ sư phần mềm Jibin Liu của eBay giải thích: “Giả sử bạn mới nuôi một chú chó con. Khi nhận ‘lệnh’ ngồi lần đầu tiên, có lẽ nó sẽ không hiểu điều này có nghĩa là gì. Cho đến cuối cùng, cứ mỗi lần nó ngồi xuống và bạn cho nó ăn. Chú chó càng luyện tập thì sẽ làm càng chính xác theo lời nói. Đây chính xác là những gì chúng tôi đang làm trong học tăng cường.”</p>
        <p>&nbsp;Học tăng cường phải cân bằng giữa khám phá (tìm ra những việc cần làm, mắc nhiều lỗi trong quá trình đó) và khai thác (thực hiện nhiều hành động hơn dẫn đến kết quả tốt hơn). Phương pháp này cũng có thể ngốn khá nhiều tính toán vì phải xem xét rất nhiều hành động tiềm năng khác nhau trước khi tìm ra hành động phù hợp. Tuy nhiên, với sức mạnh tính toán khổng lồ sẵn có hiện nay, học tăng cường đang được sử dụng cho ngày càng nhiều ứng dụng. Salesforce đã sử dụng tính năng học tăng cường để tạo ra bản tóm tắt của các tài liệu văn bản rất dài. JPMorgan đã phát triển các bot giao dịch để thực hiện giao dịch hiệu quả hơn. eBay đã sử dụng tính năng học tăng cường trên những con “nhện thông minh” để thu thập thông tin từ các trang web hiệu quả hơn và truy xuất thông tin tự động. Đã có nhiều ứng dụng của các kỹ thuật này trong y học và điều khiển robot. Sự kiện thuật toán Học Tăng cường Sâu (Deep Reinforcement Learning) đánh bại những người chơi giỏi nhất trong các trò chơi như cờ vây đã đem lại danh tiếng vang dội cho những phương pháp này.</p>
        <div style="text-align : center">
            <img src="./img/Anh27.png" alt="Anh26: phuong phap phat trien mo hinh robot" style="width : 40%"/>
            <p class = "title-img"><i>Ảnh 26: Phương pháp phát triển mô hình robot</i> </p>
        <p>Một phương pháp phổ biến để học tăng cường là Q-Learning, được Chris Watkins tạo ra vào năm 1989, lấy cảm hứng từ cách động vật và con người học hỏi từ kinh nghiệm. Phương pháp này cải thiện các hành vi tốt với những sự tăng cường mang tính tích cực và đưa ra hành động tốt nhất để thực hiện trong mọi tình huống (theo trạng thái của robot và môi trường tại thời điểm đó).&nbsp;</p>
        <p>Trong điều khiển robot, các hành động có thể là “nếu đường phía trước trống trải thì hãy tiến lên” hoặc “nếu tôi sắp va chạm với chướng ngại vật thì hãy dừng lại” (thường được gọi là các chính sách). Đó là một loại ý tưởng tương tự với các máy trạng thái hữu hạn (xem chương 3), ngoại trừ việc thay vì lập trình viên thiết kế các hành vi thì thuật toán học tăng cường sẽ tự làm việc đó. Để tối ưu hóa các hành động, thuật toán học tăng cường cần hiểu giá trị của “phần thưởng” được liên kết với mỗi hành động trong mỗi tình huống. Điều này được thực hiện bằng hàm Q, có chức năng trao phần thưởng mong đợi của một hành động (và tất cả các hành động tiếp theo) ở một trạng thái cụ thể, để chiến lược lựa chọn hành động sau đó luôn có thể chọn ra hành động tốt nhất trong chuỗi hành động và tối đa hóa tổng phần thưởng. Một phương pháp AI khác là học sâu, với đầy đủ các ví dụ, có thể học hàm Q. Khi bổ sung thuật toán này vào mạng lưới thần kinh tích chập sâu, bạn sẽ có một AI có thể nhìn thấy, tìm hiểu giá trị của các hành động đơn lẻ và chọn các hành động tốt nhất để thực hiện. Sử dụng kết hợp các AI thông minh này (và nhiều loại khác nữa), các công ty như Google Deepmind đã tạo ra những AI tự học cách chơi trò chơi điện tử giỏi hơn con người, chỉ bằng cách nhìn vào các điểm ảnh riêng lẻ trên màn hình, nhận điểm thưởng từ trò chơi và nhấn nút trên tay cầm điều khiển.&nbsp;</p>
        <p><br/></p>
        <div style="text-align : center">
            <img src="./img/Anh28.png" alt="Anh28: Geoffrey Hinton" style="width : 60%"/>
            <p class = "title-img"><i></i> </p>
        <p class = "title">Thay đổi ý tưởng&nbsp;</p>
        <p>Một số loại học không giám sát, được gọi là học trực tuyến (online learning), sẽ học liên tục để bắt kịp sự thay đổi của môi trường. Điều này rất quan trọng vì thế giới của chúng ta không bao giờ đứng yên. Nếu một quy tắc đã học được áp dụng bất kể thay đổi, sự cố có thể xảy ra. Trong một ví dụ đáng chú ý, hãng taxi công nghệ Uber đã xây dựng một quy tắc trong ứng dụng của họ là tự động tăng giá chuyến đi khi nhu cầu tăng lên. Đây có thể là một cách hiệu quả để tăng thu nhập, nhưng nó đã gây ra hậu quả khủng khiếp vào hai ngày 15–16 tháng 12 năm 2014 tại Sydney. Đây là ngày xảy ra cuộc khủng hoảng con tin ở Sydney, một tay súng đã bắt giữ 18 người làm con tin trong một quán cà phê. Một số đường phố đã bị chặn trong cuộc bao vây và nhu cầu đi Uber trong khu vực tăng đột biến, khiến giá cũng tăng theo do hệ thống định giá tự động. Thuật toán đã không biết lý do đằng sau việc tăng nhu cầu này và tuân theo quy tắc một cách mù quáng, khiến cho Uber bị chỉ trích thậm tệ trên mặt báo: có vẻ như họ đang khai thác một sự kiện khủng hoảng để kiếm tiền. (Công ty này sau đó đã hoàn lại tiền vé tăng them).</p>
        <p>&nbsp;Với thuật toán học trực tuyến, máy học có khả năng theo dõi một chuẩn mực (norm) đang thay đổi. Đó là một cách tiếp cận hữu ích trong các hệ thống phát hiện xâm nhập mạng, nơi mà xu hướng thông thường của lưu lượng truy cập Internet sẽ thay đổi theo thời gian khi mọi người nhìn vào những thứ mới trên mạng, trong khi những kẻ xâm nhập (tin tặc) cố gắng truy cập trái phép sẽ liên tục thử các thủ thuật mới để chiếm quyền điều khiển hệ thống máy tính với mục đích đánh cắp dữ liệu hoặc đòi tiền chuộc. Các hệ thống phát hiện bất thường (Anomaly detection system) được thiết kế để xử lý các vấn đề như vậy – bằng cách xây dựng một mô hình nội bộ về hành vi chuẩn mực (normal behaviour) mà chúng có thể cập nhật liên tục, trong khi phát hiện bất kỳ hành vi nào đi lệch quá mức so với chuẩn mực này. Hành vi bình thường đề cập đến hành vi mong đợi ở các cá nhân. Cách mọi người tương tác với người khác, đi về cuộc sống của họ thường phù hợp với kỳ vọng của xã hội. Khi những kỳ vọng này và hành vi cá nhân đồng bộ hóa, hành vi được coi là bình thường. Ví dụ: hãy tưởng tượng bạn thấy một cá nhân la hét tại quầy thanh toán vì nhân viên thu ngân quá chậm. Bạn sẽ không coi cá nhân là điên rồ hoặc hành vi của anh ta là bất thường. Điều này là do chúng tôi coi đó là hành vi mong đợi và bình thường của con người. Trong mọi xã hội, có những kỳ vọng xã hội, chuẩn mực, giá trị, công việc, v.v. quyết định quy tắc ứng xử của các cá nhân. Miễn là mọi người tuân thủ những điều này, hành vi của họ được coi là bình thường. Có thể có ngoại lệ cho điều này, nơi cũng có những nhân vật lập dị. Tuy nhiên, những người này không được coi là bất thường vì có sự đa dạng về tính cách và tính cách cá nhân.</p>
        <p>Nếu một mô hình hành vi đi ngược lại những gì được xã hội coi là bình thường, điều này có thể được định nghĩa là hành vi bất thường. Theo Cẩm nang thống kê chẩn đoán bất thường mô tả các rối loạn chức năng hành vi, cảm xúc, nhận thức bất ngờ trong bối cảnh văn hóa của họ và liên quan đến đau khổ cá nhân và suy giảm đáng kể trong hoạt động. Định nghĩa này cho thấy rằng những huyền thoại phổ biến mà mọi người có về những cá nhân được coi là bất thường là không chính xác. Một số huyền thoại là hành vi bất thường của cá nhân không thể được chữa khỏi và là do di truyền, họ có ý chí yếu đuối và nguy hiểm, họ không bao giờ đóng góp cho xã hội và gian xảo. Vào thời cổ đại, khi những người có hành vi bất thường được tìm thấy, họ bị coi là tham gia vào phù thủy hoặc bị quỷ ám và bị đối xử tàn bạo. Trừ tà, trepanation và liệu pháp sốc đã được đưa ra cho những người này. Trong hiện tại, bất thường được coi là một bệnh tâm thần. Trong Tâm lý học, chúng được chia thành các chủ đề khác nhau như rối loạn lâm sàng, rối loạn nhân cách, tình trạng y tế nói chung, v.v.</p>
        <p>Một số hệ thống phát hiện bất thường thậm chí còn được mô phỏng theo cách thức hoạt động của hệ thống miễn dịch của con người, vốn phải đối mặt với cùng một vấn đề. Mỗi ngày, hệ thống miễn dịch của chúng ta phải phân biệt giữa chúng ta (và chúng ta được tạo ra từ hàng nghìn tỷ tế bào và một số lượng vi khuẩn còn nhiều hơn thế trong ruột) cùng các mầm bệnh cố gắng xâm nhập cơ thể (mà nhiều trong số đó có thể là mới, vừa đột biến từ những mầm bệnh cũ). Hệ thống miễn dịch của chúng ta duy trì một tập hợp các tế bào miễn dịch và kháng thể luôn thay đổi, được thiết kế để kích hoạt bởi những kẻ xâm nhập có khả năng gây hại cho cơ thể. Các thuật toán máy tính tuân theo những ý tưởng tương tự đã mang lại một số kết quả hấp dẫn và được sử dụng để phát hiện gian lận hay thậm chí là điều khiển robot.&nbsp;</p>
        <p>Có rất nhiều kiểu học không giám sát khác nhau, mỗi kiểu có những ưu nhược điểm riêng. Một số có thể được sử dụng để tìm các biến đổi hoặc đặc tính quan trọng bị ẩn trong dữ liệu. Một số có thể xác định các hình mẫu hoặc tìm hiểu các phân cụm. Số khác thậm chí có thể tự dạy chính sách hành vi hoặc tự giám sát. Tuy nhiên, khi AI tự học, không có gì đảm bảo rằng nó sẽ hiểu được bối cảnh rộng hơn của những gì nó học được. AI có thể phân cụm dữ liệu thành các nhóm khó hiểu và các nhóm đó có thể thay đổi theo thời gian khi dữ liệu mới được cung cấp. Và trong khi học có giám sát cung cấp một ý tưởng rõ ràng và có thể đo lường được về độ chính xác, thì trong học không giám sát, khi không có ví dụ về đúng và sai để so sánh, độ chính xác có thể khó cải thiện hơn. Đó là một vấn đề thú vị của AI nhưng vẫn chưa được giải quyết. Yann LeCun, Giám đốc Khoa học về AI tại Facebook cho biết: “Hầu hết việc học của con người và động vật là học không giám sát. Nếu trí tuệ là một chiếc bánh, thì học không giám sát sẽ là cốt bánh, học có giám sát là lớp kem phủ ngoài và học tăng cường là quả anh đào trên chiếc bánh. Chúng ta biết cách làm kem và anh đào, nhưng không biết làm cốt bánh.”</p>
        <p> </p>
    </div>

<div><br/></div>
</body>
</html>